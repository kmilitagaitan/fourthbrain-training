{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lnMVXWu2yoG"
   },
   "source": [
    "# Today you are a Machine Learning Engineer at the Department of New Products at Target Cosmetics!\n",
    "This work relies on processed data from Kaggle https://www.kaggle.com/mkechinov/ecommerce-events-history-in-cosmetics-shop\n",
    "\n",
    "This work is motivated by the publication https://arxiv.org/pdf/2010.02503.pdf\n",
    "\n",
    "So far you have seen user-product interaction data that can lead to classification of a user-product relationship as ending in purchase or no-purchase, and for clustering (categorizing) user behaviors.\n",
    "\n",
    "In this assignment, we will have a very small training set to work with. Additionally, the test set we'll use has very few features. We'll first expose you to an Auto-Machine Learning library called TPOT and show you how it can be used to search over many ML model architectures. Then we will use the Label Spreading method to do semi-supervised learning, allowing us to leverage a small amount of labeled data in combination with a larger amount of unlabeled data. Finally we'll have a more open-ended task centering on system design for Zero-shot learning.\n",
    "\n",
    "Labeled data is sparse, and in our hypothetical application, (cosmetics purchase prediction) the intention is to maximize Recall (so that no popular cosmetic is understocked). Digital overstocking is allowed since it will not cause disengagement in customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Exploratory Data Analysis (EDA) and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read in the data file `Past_month_products.csv` and save it as a DataFrame called `past_df`. This dataset has the past data and will be our training set.\n",
    "\n",
    "    Look at the shape of the DataFrame to determine the number of features and number of datapoints.\n",
    "    \n",
    "    Look at the first few rows of the DataFrame and review the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "id": "lfwUwPtq2yoR",
    "outputId": "0aefccdf-754f-45e5-de72-09f004b1df4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(5000, 37)"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "past_df = pd.read_csv('data/Past_month_products.csv')\n",
    "past_df.head()\n",
    "past_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Read in the data in `Next_month_products.csv` and save it as a DataFrame called `next_df`. This is the test dataset.\n",
    "\n",
    "    Look at the shape of the DataFrame and look at the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "vW5WsvMM2yoU",
    "outputId": "649b5a64-a5a9-4f5b-89f8-89aefaa0daa1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(30091, 5)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_df = pd.read_csv('data/Next_month_products.csv')\n",
    "next_df.head()\n",
    "next_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How does the number of datapoints in the training set compare to the number of datapoints in the test set?\n",
    "\n",
    "    And how does the feature set in the training set compare to the feature set in the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "There is a difference of 25091 datapoints between the training and the testing dataset. Also there is a difference of 32 features between the train/test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRrXSVcM2yoO"
   },
   "source": [
    "Imagine that you are helping plan the launch of new products. You have to figure out how to mine the past cosmetic sales data from last month, utilize relevant features and to make estimations as to which products will sell more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What percentage of datapoints are a purchase in the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.38\n"
     ]
    }
   ],
   "source": [
    "#purchase_df = past_df.loc[past_df['Purchase'] == 1]\n",
    "print(past_df['Purchased?'].mean() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. What percentage of datapoints are a purchase in the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.42557575354757\n"
     ]
    }
   ],
   "source": [
    "print(next_df['Purchased?'].mean() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Are there any product ids in both the training and test datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of overlapping ids:  0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "unique_train_ids = np.unique(past_df['product_id'])\n",
    "unique_test_ids = np.unique(next_df['product_id'])\n",
    "\n",
    "intersection = len(set(np.unique(unique_train_ids)).intersection(set(unique_test_ids)))\n",
    "\n",
    "print('Number of overlapping ids: ', intersection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Create `X_train`, `y_train`, `X_test`, and `y_test` according to the following guidelines.\n",
    "    * The `Purchased?` column is the target.\n",
    "    * `X_train` and `X_test` should contain the same features (so you will not be able to use all the features).\n",
    "    * `product_id` should not be a feature.\n",
    "    \n",
    "    Double check that the shapes of the four arrays are what you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S4ltwfa62yoa",
    "outputId": "0fb26318-9427-40ce-fcef-83b5557f70fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 3) (5000,) (30091, 3) (30091,)\n"
     ]
    }
   ],
   "source": [
    "X_train = past_df[['maxPrice',\t'minPrice',\t'Category']].values\n",
    "y_train = past_df['Purchased?'].values\n",
    "\n",
    "X_test = next_df[['maxPrice',\t'minPrice',\t'Category']].values\n",
    "y_test = next_df['Purchased?'].values\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOPpOWog2yoc"
   },
   "source": [
    "## Task 2: Build the best classifier you can using only the Past month's data.\n",
    "\n",
    "We will be using the TPOT library to find an optimal model.\n",
    "\n",
    "1. Install `tpot`.\n",
    "\n",
    "    If you're running the notebook locally, follow [these instructions](https://epistasislab.github.io/tpot/installing/), using either conda or pip.\n",
    "    \n",
    "    If you're using Colab, uncomment the following line to install tpot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xynRq9x4QlJL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tpot in /Users/camilagaitanmosquera/opt/anaconda3/envs/tensorflow-env/lib/python3.7/site-packages (0.11.7)\r\n",
      "Requirement already satisfied: update-checker>=0.16 in /Users/camilagaitanmosquera/opt/anaconda3/envs/tensorflow-env/lib/python3.7/site-packages (from tpot) (0.18.0)\r\n",
      "Requirement already satisfied: xgboost>=1.1.0 in /Users/camilagaitanmosquera/opt/anaconda3/envs/tensorflow-env/lib/python3.7/site-packages (from tpot) (1.5.2)\r\n",
      "Requirement already satisfied: scikit-learn>=0.22.0 in /Users/camilagaitanmosquera/opt/anaconda3/envs/tensorflow-env/lib/python3.7/site-packages (from tpot) (1.0.2)\r\n",
      "Requirement already satisfied: tqdm>=4.36.1 in /Users/camilagaitanmosquera/opt/anaconda3/envs/tensorflow-env/lib/python3.7/site-packages (from tpot) (4.62.3)\r\n",
      "Requirement already satisfied: numpy>=1.16.3 in /Users/camilagaitanmosquera/opt/anaconda3/envs/tensorflow-env/lib/python3.7/site-packages (from tpot) (1.19.5)\r\n",
      "Requirement already satisfied: stopit>=1.1.1 in /Users/camilagaitanmosquera/opt/anaconda3/envs/tensorflow-env/lib/python3.7/site-packages (from tpot) (1.1.2)\r\n",
      "Requirement already satisfied: pandas>=0.24.2 in /Users/camilagaitanmosquera/opt/anaconda3/envs/tensorflow-env/lib/python3.7/site-packages (from tpot) (1.3.4)\r\n",
      "Requirement already satisfied: scipy>=1.3.1 in /Users/camilagaitanmosquera/opt/anaconda3/envs/tensorflow-env/lib/python3.7/site-packages (from tpot) (1.7.3)\r\n",
      "Requirement already satisfied: deap>=1.2 in /Users/camilagaitanmosquera/opt/anaconda3/envs/tensorflow-env/lib/python3.7/site-packages (from tpot) (1.3.1)\r\n",
      "Requirement already satisfied: joblib>=0.13.2 in /Users/camilagaitanmosquera/opt/anaconda3/envs/tensorflow-env/lib/python3.7/site-packages (from tpot) (1.1.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/camilagaitanmosquera/opt/anaconda3/envs/tensorflow-env/lib/python3.7/site-packages (from pandas>=0.24.2->tpot) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/camilagaitanmosquera/opt/anaconda3/envs/tensorflow-env/lib/python3.7/site-packages (from pandas>=0.24.2->tpot) (2021.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/camilagaitanmosquera/opt/anaconda3/envs/tensorflow-env/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas>=0.24.2->tpot) (1.16.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/camilagaitanmosquera/opt/anaconda3/envs/tensorflow-env/lib/python3.7/site-packages (from scikit-learn>=0.22.0->tpot) (2.2.0)\r\n",
      "Requirement already satisfied: requests>=2.3.0 in /Users/camilagaitanmosquera/opt/anaconda3/envs/tensorflow-env/lib/python3.7/site-packages (from update-checker>=0.16->tpot) (2.27.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/camilagaitanmosquera/opt/anaconda3/envs/tensorflow-env/lib/python3.7/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2021.10.8)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/camilagaitanmosquera/opt/anaconda3/envs/tensorflow-env/lib/python3.7/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (1.26.8)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/camilagaitanmosquera/opt/anaconda3/envs/tensorflow-env/lib/python3.7/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/camilagaitanmosquera/opt/anaconda3/envs/tensorflow-env/lib/python3.7/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (3.3)\r\n"
     ]
    }
   ],
   "source": [
    " !pip install tpot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Instantiate and train a TPOT auto-ML classifier.\n",
    "\n",
    "    The parameters are set fairly aritrarily (with some trial and error). Use these parameter values:\n",
    "    * `generations`: 5\n",
    "    * `population_size`: 40\n",
    "    * `verbosity`: 2 (so you can see each generation's performance)\n",
    "    \n",
    "    The final line with create a Python script `tpot_products_pipeline.py` with the code to create the optimal model found by TPOT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346,
     "referenced_widgets": [
      "f3bd13b258f04f5dae56c6587e222332",
      "144349e1d310495b95390340061bc351",
      "7991c5e487244aff8c0aeac941d0b906",
      "c3072ae59260462798b87f4706ff897b",
      "74454df9d33c41b68351e61957578af2",
      "42a91e12f6724c5fadc53c17a965c982",
      "21bf14b1a90f4a7abd2e2da3784adfc4",
      "9ea3bac4950545ca901272a48eaf45ea"
     ]
    },
    "id": "0Z0aR02P2yod",
    "outputId": "8d8dc915-c09c-448b-9415-c110b72f4501"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/camilagaitanmosquera/opt/anaconda3/envs/tensorflow-env/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: xgboost.XGBClassifier is not available and will not be used by TPOT.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Optimization Progress:   0%|          | 0/300 [00:00<?, ?pipeline/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eb5bf9291f7a491486d0bee13040a099"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.8724000000000001\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.8724000000000001\n",
      "\n",
      "Generation 3 - Current best internal CV score: 0.8747999999999999\n",
      "\n",
      "Generation 4 - Current best internal CV score: 0.8747999999999999\n",
      "\n",
      "Generation 5 - Current best internal CV score: 0.8747999999999999\n",
      "\n",
      "Best pipeline: DecisionTreeClassifier(SelectFromModel(input_matrix, criterion=entropy, max_features=0.2, n_estimators=100, threshold=0.30000000000000004), criterion=entropy, max_depth=8, min_samples_leaf=13, min_samples_split=17)\n",
      "0.8721544647901366\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, random_state=42)\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export('tpot_products_pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Take the appropriate lines (updating the variable names) from `tpot_products_pipeline.py` to build a model on our training set and make predictions on the test set. Save the predictions as `y_pred`.\n",
    "\n",
    "    If there is model used in `tpot_products_pipeline.py` that you aren't familiar with, look it up!\n",
    "\n",
    "    Note: There is randomness to the way the TPOT searches, so it's possible you won't have exactly the same result as your classmate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wm5Ac97r2yog",
    "outputId": "b621a708-1259-4c59-ee47-8a884c933c54"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tpot.export_utils import set_param_recursive\n",
    "\n",
    "# Average CV score on the training set was: 0.8747999999999999\n",
    "exported_pipeline = make_pipeline(\n",
    "    SelectFromModel(estimator=ExtraTreesClassifier(criterion=\"entropy\", max_features=0.2, n_estimators=100), threshold=0.30000000000000004),\n",
    "    DecisionTreeClassifier(criterion=\"entropy\", max_depth=8, min_samples_leaf=13, min_samples_split=17)\n",
    ")\n",
    "# Fix random state for all the steps in exported pipeline\n",
    "set_param_recursive(exported_pipeline.steps, 'random_state', 42)\n",
    "\n",
    "exported_pipeline.fit(X_train, y_train)\n",
    "y_pred = exported_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diQyYboWIb0u"
   },
   "source": [
    "4. Compute some evaluation metrics for the predictions made above. Print the accuracy, recall, precision, f1 score and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fbGFSnzi2yoi",
    "outputId": "dc23c192-77f3-49c0-e3e6-0eb9f8ddccb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8721544647901366, Precision = 0.9604072398190046, Recall = 0.6556617434115262, F1-score = 0.7793012449084964\n",
      "Confusion Matrix is:\n",
      "[[19452   280]\n",
      " [ 3567  6792]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "from sklearn.metrics import recall_score as recall\n",
    "from sklearn.metrics import precision_score as precision\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "cmtp=confusion_matrix(y_test, y_pred)\n",
    "acc  = accuracy(y_test, y_pred)\n",
    "rec  = recall(y_test, y_pred)\n",
    "prec = precision(y_test, y_pred)\n",
    "f1   = f1_score(y_test, y_pred)\n",
    "print(f'Accuracy = {acc}, Precision = {prec}, Recall = {rec}, F1-score = {f1}')\n",
    "print('Confusion Matrix is:')\n",
    "print(cmtp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-px8SPVv2yol"
   },
   "source": [
    "## Task 3: Semi-supervised learning: Apply label spreading on the data.\n",
    "\n",
    "We won't use any of the labels for the test set. We'll just use labels for the training set. We will, however, use the **features** from the test set along with the features from the training set. Since we're using a large number of sampled features, but only a small number of these samples have labels, this is **semi-supervised learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31i6GDQCJsdd"
   },
   "source": [
    "1. Create a matrix `X` that has the rows from `X_train` concatenated with the rows from `X_test`.\n",
    "\n",
    "    Check the shape of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OyxJD6So2yol",
    "outputId": "728e9343-e750-4b35-f9dc-e64c76c59519"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(35091, 3)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.concatenate([X_train, X_test])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31i6GDQCJsdd"
   },
   "source": [
    "2. Create the target array `y` by concatenating `y_train` with a vector of -1's, effectively creating a dummy label for the `X_test` rows in `X`.\n",
    "\n",
    "    Check the shape of the array. It should have as many values as `X` has rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "(35091,)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.concatenate([y_train, -np.ones(X_test.shape[0])])\n",
    "y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZ9YdmHQKFBB"
   },
   "source": [
    "Scikit-learn provides two label propagation models: `LabelPropagation` and `LabelSpreading`. Both work by constructing a similarity graph over all items in the input dataset. LabelSpreading is similar to the basic Label Propagation algorithm, but it uses an affinity matrix based on the normalized graph Laplacian and soft clamping across the labels. We will be using scikit-learn's `LabelSpreading` model with `kNN`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glrGziOPVPA3"
   },
   "source": [
    "3. Train a `LabelSpreading` model. Set `kernel` to `knn` and `alpha` to 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q4XOZHrqVQo0",
    "outputId": "a5b6c4dc-85cf-46fc-da58-7675855b0e03"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "LabelSpreading(alpha=0.01, kernel='knn')"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.semi_supervised import LabelSpreading\n",
    "model = LabelSpreading(kernel='knn', alpha=0.01)\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUw9UyOxKRkS"
   },
   "source": [
    "4. Extract the predictions for the test data. You can get the predictions from the `transduction_` attribute. Note that there is a value for every row in `X`, so select just the values that correspond to `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "bOE-d_R02you"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(30091,)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.transduction_\n",
    "y_pred =  y_pred[y_train.shape[0]:]\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diQyYboWIb0u"
   },
   "source": [
    "5. Compute some evaluation metrics for the predictions. Print the accuracy, recall, precision, f1 score and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fbGFSnzi2yoi",
    "outputId": "dc23c192-77f3-49c0-e3e6-0eb9f8ddccb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8184174670167159, Precision = 0.7975683890577507, Recall = 0.6332657592431702, F1-score = 0.7059836418424451\n",
      "Confusion Matrix is:\n",
      "[[18067  1665]\n",
      " [ 3799  6560]]\n"
     ]
    }
   ],
   "source": [
    "cmtp=confusion_matrix(y_test, y_pred)\n",
    "acc  = accuracy(y_test, y_pred)\n",
    "rec  = recall(y_test, y_pred)\n",
    "prec = precision(y_test, y_pred)\n",
    "f1   = f1_score(y_test, y_pred)\n",
    "print(f'Accuracy = {acc}, Precision = {prec}, Recall = {rec}, F1-score = {f1}')\n",
    "print('Confusion Matrix is:')\n",
    "print(cmtp)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3HziHwE2yoz"
   },
   "source": [
    "6. Collect your results in the table below to compare the two models.\n",
    "\n",
    "\n",
    "| Method          | Recall       | F1-score     | Accuracy     |\n",
    "| --------------- | ------------ | ------------ | ------------ |\n",
    "| TPOT (AutoML)   |   0.6556617434115262           |     0.7793012449084964         |    0.8721544647901366          |\n",
    "| Label Spreading |      0.6332657592431702        |    0.7059836418424451        |   0.8184174670167159  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Unmp-DRM2yoz"
   },
   "source": [
    "## Task 4: System Design for Zero Shot Learning:\n",
    "So far we have been looking at 3 product level features (min price, max price, Product Category) to classify if a particular product will get get purchased or not.\n",
    "Now, let's say you have access to some more information regarding each Past sold cosmetic item and the Next cosmetic item. Design a System to enable accurate identification of an item that is more likely to be purchased.\n",
    "Think through the following:\n",
    "1. What additional data fields do you need per cosmetic in past and Next catalogue? How would you process these data fields?\n",
    "2. You have access to picture images of each cosmetic. How will you use these images to extract relevant features for gauging interest in the new cosmetics?\n",
    "3. Design an end-to-end system workflow using the additional cosmetic data and cosmetic images to predict its purchasing polularity."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**This task  was optional for this assigment**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0v931vvX0PR"
   },
   "source": [
    "## Task 5: Summary and Discussion\n",
    "\n",
    "What would you report back as the best method to gauge product popularity?\n",
    "\n",
    "Think in terms of Data, Process and Outcomes specifically.\n",
    "\n",
    "Consider the following:\n",
    "1. Can you store the data in some other way to enable ZSL or more efficient information storage/retrieval?\n",
    "2. Given a new data set on the job, how would you report the best \"method\"? What are the steps to always follow? \n",
    "3. What is the metric/metrics you would use to report your results?\n",
    "\n",
    "Share screen and discuss findings. Think about generalizability (something that works across data sets)\n",
    "\n",
    "Also, look into ML system design in terms of Data, Process and Outcome."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Categorizing_New_Products_At_Target_Cosmetics_SOLUTION.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "144349e1d310495b95390340061bc351": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "21bf14b1a90f4a7abd2e2da3784adfc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "42a91e12f6724c5fadc53c17a965c982": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74454df9d33c41b68351e61957578af2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "7991c5e487244aff8c0aeac941d0b906": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "Optimization Progress: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_42a91e12f6724c5fadc53c17a965c982",
      "max": 240,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_74454df9d33c41b68351e61957578af2",
      "value": 240
     }
    },
    "9ea3bac4950545ca901272a48eaf45ea": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3072ae59260462798b87f4706ff897b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9ea3bac4950545ca901272a48eaf45ea",
      "placeholder": "​",
      "style": "IPY_MODEL_21bf14b1a90f4a7abd2e2da3784adfc4",
      "value": " 240/240 [09:54&lt;00:00,  9.87s/pipeline]"
     }
    },
    "f3bd13b258f04f5dae56c6587e222332": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7991c5e487244aff8c0aeac941d0b906",
       "IPY_MODEL_c3072ae59260462798b87f4706ff897b"
      ],
      "layout": "IPY_MODEL_144349e1d310495b95390340061bc351"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}